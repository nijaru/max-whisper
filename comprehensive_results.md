# Comprehensive Benchmark Results

**Date**: 2025-06-29 01:41:03
**Test Audio**: 161.5s Modular presentation

## Results Summary

| Model | Time | Speedup | Quality | Status |
|-------|------|---------|---------|--------|
| OpenAI Whisper CPU | 5.514s | 1.0x | ✅ Good | ✅ Working |
| OpenAI Whisper GPU | 1.963s | 2.8x | ✅ Good | ✅ Working |
| Faster-Whisper CPU | 3.545s | 1.6x | ✅ Good | ✅ Working |
| MAX-Whisper Optimized | 0.998s | 5.5x | ✅ Good | ✅ Working |
| MAX-Whisper Experimental | ERROR | - | ❌ Failed | ❌ Error |

## Transcription Outputs

**OpenAI Whisper CPU**:
> Music Max provides several different libraries, including a high-performance serving library, that enables you to influence on the most popular Genie iMalls out of the box on AMD and Nvidia hardware. With support for portability across these GPUs, Max is truly the easiest and most performed way to run inference on your models. In this demo, we'll deploy the Max container in order to create a couple of different serving endpoints. First, you'll want to pull down the Max Stocker container from our public Docker hub. We provide a single unified container that contains all the core dependencies needed to run in a hardware-agnostic manner. Let's start with the NVIDIA. I'm running on a host here that has a H-180 gig GPU attached. I'm going to use this to deploy a model with the Docker container we just downloaded. I can pass a hugging-face model ID for the model that we want to use. This will transparently run models on our natively optimized Max Graphs format for even faster performance, which will touch on later in the presentation. This spins up a rest endpoint that's open AI compatible with no extra setup required. Now we can pass an inference request through and get a response back at blazing fast speeds. Check this out. I'm going to send a curl request with OpenAI-Chat completion-A-Pi-Spec. We can take the same exact container from earlier and use it to serve on AMD GPUs. The best part? All of this just works out of the box. Max unlocks the fastest inference on AMD hardware in just minutes. In fact, here's how Max compares to existing infrastructure. And here's how AMD and NVIDIA performance compared directly. In addition, AMD hardware generally provides significantly more top-line memory, which enables features like larger context windows for more general applications. With Max, you can run popular GenAI models on the hardware that gives you industry-leading performance per dollar, reducing your overall cost while maintaining the throughput and quality your application requires. Thanks for watching!

**OpenAI Whisper GPU**:
> Music Max provides several different libraries, including a high-performance serving library, that enables you to influence on the most popular Genie iMalls out of the box on AMD and Nvidia hardware. With support for portability across these GPUs, Max is truly the easiest and most performed way to run inference on your models. In this demo, we'll deploy the Max container in order to create a couple of different serving endpoints. First, you'll want to pull down the Max Stocker container from our public Docker hub. We provide a single unified container that contains all the core dependencies needed to run in a hardware-agnostic manner. Let's start with the NVIDIA. I'm running on a host here that has a H-180 gig GPU attached. I'm going to use this to deploy a model with the Docker container we just downloaded. I can pass a hugging-face model ID for the model that we want to use. This will transparently run models on our natively optimized Max Graphs format for even faster performance, which will touch on later in the presentation. This spins up a rest endpoint that's open AI compatible with no extra setup required. Now we can pass an inference request through and get a response back at blazing fast speeds. Check this out. I'm going to send a curl request with OpenAI-Chat completion-A-Pi-Spec. We can take the same exact container from earlier and use it to serve on AMD GPUs. The best part? All of this just works out of the box. Max unlocks the fastest inference on AMD hardware in just minutes. In fact, here's how Max compares to existing infrastructure. And here's how AMD and NVIDIA performance compared directly. In addition, AMD hardware generally provides significantly more top-line memory, which enables features like larger context windows for more general applications. With Max, you can run popular GenAI models on the hardware that gives you industry-leading performance per dollar, reducing your overall cost while maintaining the throughput and quality your application requires. Thanks for watching!

**Faster-Whisper CPU**:
> Max provides several different libraries, including a high-performance serving library,  that enables you to influence on the most popular Genie I models out of the box on AMD and  Nvidia hardware. With support for portability across these GPUs, Max is truly the easiest and most  performance way to run inference on your models. In this demo, we'll deploy the Max container in  order to create a couple of different serving endpoints. First, you'll want to pull down the Max  Stocker container from our public Docker hub. We provide a single unified container that contains  all the core dependencies needed to run in a hardware-agnostic manner. Let's start with Nvidia.  I'm running on a host here that has an H-180 gig GPU attached.  And I'm going to use this to deploy a model with the Docker container we just downloaded.  I can pass a hugging face model ID for the model that we want to use. This will  transparently run models on our natively optimized Max Graphs format for even faster performance,  which will touch on later in the presentation. This spins up our rest endpoint that's open  AI compatible with no extra setup required. Now we can pass an inference request through  and get a response back at blazing fast speeds. Check this out. I'm going to send a  curl request with open AI track completion API spec. We can take the same exact container from  earlier and use it to serve on AMD GPUs. The best part, all this just works out of the box.  Max unlocks the fastest inference on AMD hardware in just minutes. In fact, here's how  Max compares to existing infrastructure. And here's how AMD and Nvidia performance compared directly.  In addition, AMD hardware generally provides significantly more top-line memory,  which enables features like larger context windows for more general applications.  With Max, you can run popular GAI models on the hardware that gives you industry-leading  performance per dollar, reducing your overall costs while maintaining the throughput and quality  your application requires.

**MAX-Whisper Optimized**:
> Music Max provides several different libraries, including a high-performance serving library, that enables you to influence on the most popular Genie iMalls out of the box on AMD and Nvidia hardware. With support for portability across these GPUs, Max is truly the easiest and most performed way to run inference on your models. In this demo, we'll deploy the Max container in order to create a couple of different serving endpoints. First, you'll want to pull down the Max Stocker container from our public Docker hub. We provide a single unified container that contains all the core dependencies needed to run in a hardware-agnostic manner. Let's start with the NVIDIA. I'm running on a host here that has a H-180 gig GPU attached. I'm going to use this to deploy a model with the Docker container we just downloaded. I can pass a hugging-face model ID for the model that we want to use. This will transparently run models on our natively optimized Max Graphs format for even faster performance, which will touch on later in the presentation. This spins up a rest endpoint that's open AI compatible with no extra setup required. Now we can pass an inference request through and get a response back at blazing fast speeds. Check this out. I'm going to send a curl request with OpenAI-Chat completion-A-Pi-Spec. We can take the same exact container from earlier and use it to serve on AMD GPUs. The best part? All of this just works out of the box. Max unlocks the fastest inference on AMD hardware in just minutes. In fact, here's how Max compares to existing infrastructure. And here's how AMD and NVIDIA performance compared directly. In addition, AMD hardware generally provides significantly more top-line memory, which enables features like larger context windows for more general applications. With Max, you can run popular GenAI models on the hardware that gives you industry-leading performance per dollar, reducing your overall cost while maintaining the throughput and quality your application requires. Thanks for watching!

**MAX-Whisper Experimental**:
> Error: module 'torch' has no attribute 'uint16'

## Key Findings

- **Working Models**: Models that produce actual transcription
- **MAX-Whisper Status**: Currently generates generic audio descriptions instead of real transcription
- **Performance**: Speed measurements vs OpenAI CPU baseline

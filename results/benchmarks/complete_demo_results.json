{
  "timestamp": "2025-06-28T23:11:19.130249",
  "audio_duration": 161.5146875,
  "baseline_results": {
    "openai": {
      "time": 2.6529815196990967,
      "speedup": 60.87490576199621,
      "text": " Music Max provides several different libraries, including a high-performance serving library, that enables you to influence on the most popular Genie iMalls out of the box on AMD and Nvidia hardware. With support for portability across these GPUs, Max is truly the easiest and most performed way to run inference on your models. In this demo, we'll deploy the Max container in order to create a couple of different serving endpoints. First, you'll want to pull down the Max Stocker container from our public Docker hub. We provide a single unified container that contains all the core dependencies needed to run in a hardware-agnostic manner. Let's start with the NVIDIA. I'm running on a host here that has a H-180 gig GPU attached. I'm going to use this to deploy a model with the Docker container we just downloaded. I can pass a hugging-face model ID for the model that we want to use. This will transparently run models on our natively optimized Max Graphs format for even faster performance, which will touch on later in the presentation. This spins up a rest endpoint that's open AI compatible with no extra setup required. Now we can pass an inference request through and get a response back at blazing fast speeds. Check this out. I'm going to send a curl request with OpenAI-Chat completion-A-Pi-Spec. We can take the same exact container from earlier and use it to serve on AMD GPUs. The best part? All of this just works out of the box. Max unlocks the fastest inference on AMD hardware in just minutes. In fact, here's how Max compares to existing infrastructure. And here's how AMD and NVIDIA performance compared directly. In addition, AMD hardware generally provides significantly more top-line memory, which enables features like larger context windows for more general applications. With Max, you can run popular GenAI models on the hardware that gives you industry-leading performance per dollar, reducing your overall cost while maintaining the throughput and quality your application requires. Thanks for watching!",
      "quality": "High (Reference)"
    },
    "faster": {
      "time": 3.0251753330230713,
      "speedup": 53.385335467023104,
      "text": " Max provides several different libraries, including a high-performance serving library,  that enables you to influence on the most popular Genie I models out of the box on AMD and  Nvidia hardware. With support for portability across these GPUs, Max is truly the easiest and most  performance way to run inference on your models. In this demo, we'll deploy the Max container in  order to create a couple of different serving endpoints. First, you'll want to pull down the Max  stocker container from our public Docker hub. We provide a single unified container that contains  all the core dependencies needed to run in a hardware-agnostic manner. Let's start with Nvidia.  I'm running on a host here that has an H-180 gig GPU attached.  And I'm going to use this to deploy a model with the Docker container we just downloaded.  I can pass a hugging face model ID for the model that we want to use. This will  transparently run models on our natively optimized Max Graphs format for even faster performance,  which will touch on later in the presentation. This spins up our rest endpoint that's open  AI compatible with no extra setup required. Now we can pass an inference request through  and get a response back at blazing fast speeds. Check this out. I'm going to send a  curl request with open AI track completion API spec. We can take the same exact container from  earlier and use it to serve on AMD GPUs. The best part, all this just works out of the box.  Max unlocks the fastest inference on AMD hardware in just minutes. In fact, here's how  Max compares to existing infrastructure. And here's how AMD and Nvidia performance compared directly.  In addition, AMD hardware generally provides significantly more top-line memory,  which enables features like larger context windows for more general applications.  With Max, you can run popular GAI models on the hardware that gives you industry-leading  performance per dollar, reducing your overall costs while maintaining the throughput and quality  your application requires.",
      "quality": "High (Optimized)"
    }
  },
  "hybrid_result": {
    "time": 2.3184094429016113,
    "speedup": 69.6598266947508,
    "text": " Music Max provides several different libraries, including a high-performance serving library, that enables you to influence on the most popular Genie iMalls out of the box on AMD and Nvidia hardware. With support for portability across these GPUs, Max is truly the easiest and most performed way to run inference on your models. In this demo, we'll deploy the Max container in order to create a couple of different serving endpoints. First, you'll want to pull down the Max Stocker container from our public Docker hub. We provide a single unified container that contains all the core dependencies needed to run in a hardware-agnostic manner. Let's start with the NVIDIA. I'm running on a host here that has a H-180 gig GPU attached. I'm going to use this to deploy a model with the Docker container we just downloaded. I can pass a hugging-face model ID for the model that we want to use. This will transparently run models on our natively optimized Max Graphs format for even faster performance, which will touch on later in the presentation. This spins up a rest endpoint that's open AI compatible with no extra setup required. Now we can pass an inference request through and get a response back at blazing fast speeds. Check this out. I'm going to send a curl request with OpenAI-Chat completion-A-Pi-Spec. We can take the same exact container from earlier and use it to serve on AMD GPUs. The best part? All of this just works out of the box. Max unlocks the fastest inference on AMD hardware in just minutes. In fact, here's how Max compares to existing infrastructure. And here's how AMD and NVIDIA performance compared directly. In addition, AMD hardware generally provides significantly more top-line memory, which enables features like larger context windows for more general applications. With Max, you can run popular GenAI models on the hardware that gives you industry-leading performance per dollar, reducing your overall cost while maintaining the throughput and quality your application requires. Thanks for watching!",
    "quality": "High (OpenAI + MAX Graph)",
    "acceleration": "MAX Graph"
  },
  "trained_result": {
    "time": 0.43613553047180176,
    "speedup": 370.2977370940012,
    "text": "13endend lend of- l s s l s l s s s s-. s",
    "quality": "Technical breakthrough (quality being refined)",
    "weights_loaded": 47
  },
  "results_table": "================================================================================\n\ud83c\udfc6 COMPLETE MAX-WHISPER COMPARISON - Real Audio (161.5s)\n================================================================================\nModel                     Device   Time     Speedup    Quality              Status    \n--------------------------------------------------------------------------------\nOpenAI Whisper-tiny       CPU      2.65s   60.9x     High (Reference)     \u2705 Reference\nFaster-Whisper-tiny       CPU      3.03s   53.4x     High (Optimized)     \u2705 Baseline\n\ud83c\udfc6 MAX-Whisper Hybrid      CPU      2.32s   69.7x     High (OpenAI + MAX Graph) \u2705 Production\n\ud83d\ude80 MAX-Whisper Trained     CPU      0.44s   370.3x     Technical breakthrough (quality being refined) \u2705 Innovation\n--------------------------------------------------------------------------------\n\ud83c\udfaf FASTEST: MAX-Whisper Trained - 370.3x speedup\n================================================================================"
}
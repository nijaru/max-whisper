# Technical Notes

## Architecture Overview

### Current Pipeline Structure (Hybrid Implementation)
```
Audio â†’ Mel Spectrogram â†’ MAX Graph Encoder â†’ PyTorch Decoder â†’ Text
                           â†“ (47ms, 99.99% similarity)    â†“ (meaningful output)
                     Complete encoder implementation    Partial transcription
```

### Future Full MAX Graph Pipeline
```
Audio â†’ Mel Spectrogram â†’ MAX Graph Encoder â†’ MAX Graph Decoder â†’ Text
                           â†“                    â†“
                    All MAX Graph operations (target architecture)
```

**MAJOR ACHIEVEMENTS**: 
- âœ… Fixed mel spectrogram preprocessing (whisper.log_mel_spectrogram vs librosa.power_to_db)
- âœ… Implemented proper NHWC/RSCF layout for MAX Graph Conv2D operations
- âœ… Achieved 99.99% cosine similarity with OpenAI encoder features
- âœ… Successful cross-framework integration (MAX Graph â†’ PyTorch)

### Implementation Files
- `max-whisper/whisper_cpu.py` - Reference OpenAI Whisper implementation
- `max-whisper/whisper_gpu.py` - CUDA-accelerated version  
- `max-whisper/whisper_max.py` - MAX Graph encoder hybrid

## MAX Graph Integration Details

### Device Setup Pattern
```python
from max.driver import accelerator_count, Accelerator, CPU, DeviceRef, InferenceSession

if accelerator_count() > 0:
    driver_device = Accelerator()
    device = DeviceRef.GPU()
else:
    driver_device = CPU()
    device = DeviceRef.CPU()

session = InferenceSession(devices=[driver_device])
```

### Working Operations
```python
from max.graph import ops

# Verified working operations:
ops.matmul(a, b)           # Matrix multiplication
ops.transpose(x, 0, 1)     # Tensor transpose
ops.layer_norm(x, w, b)    # Layer normalization  
ops.gelu(x)                # GELU activation
ops.slice_tensor(x, [...]) # Tensor slicing
```

### Weight Extraction
Successfully extracts 65 weights from Whisper tiny model:
- Convolutional layers: conv1_weight (384,80,3), conv2_weight (384,384,3)
- Transformer layers: 4 layers Ã— 16 weights each (attention + MLP)
- Position embeddings and layer norms

## Investigation Findings

### What Works
1. **Compilation**: Graph compiles without errors using proper input types
2. **Execution**: Encoder processes in ~123ms on GPU with no exceptions
3. **Integration**: Tensor conversions between MAX Graph and PyTorch work correctly
4. **Performance**: Encoder execution is significantly faster than CPU baseline

### Current Status: Hybrid Implementation Working âœ…
**Encoder Achievement**: 99.99% cosine similarity with OpenAI (mean: 0.031, std: 1.448)
**Performance**: 47ms encoder execution (23x faster than CPU encoder alone)
**Integration**: Successful MAX Graph encoder â†’ PyTorch decoder pipeline
**Output Quality**: Meaningful but partial transcription (218 vs 2035 chars expected)

**Technical Breakthrough**: Complete encoder semantic preservation achieved through:
1. **Mel Preprocessing Fix**: Using whisper.log_mel_spectrogram() instead of librosa.power_to_db()
2. **NHWC Layout**: Proper Conv2D implementation with NHWC input and RSCF weight format
3. **Weight Format**: Correct permutation for MAX Graph convolution operations
4. **Cross-Framework**: Robust tensor conversion between MAX Graph and PyTorch

### Current Challenge: Decoder Optimization
**Issue**: Decoder stops generating after 218 characters despite near-perfect encoder features
**Root Cause**: Subtle feature differences affecting decoder confidence and stopping criteria
**Next Steps**: Parameter tuning and decoder behavior analysis

### Architecture Components
1. **Encoder (MAX Graph)**: Complete implementation with 99.99% fidelity âœ…
2. **Decoder (PyTorch)**: Working but needs optimization for full-length output ðŸ”§
3. **Future Decoder (MAX Graph)**: Research phase for full native implementation ðŸ“‹

## Key Reference Materials
- `external/modular/examples/pytorch_custom_ops/whisper.py` - Modular's attention example
- `external/modular/max/graph/ops/` - MAX Graph operation implementations
- OpenAI Whisper encoder architecture for comparison

## Development Environment
- **Setup**: `make install` (pixi environment)
- **Testing**: `pixi run -e benchmark demo` (enhanced comparison UI)
- **Benchmarking**: `pixi run -e benchmark benchmark-json` (structured output)
- **Unit Tests**: `pixi run test` (comprehensive test suite)
- **Environment**: Use `pixi run -e benchmark` prefix for all commands
- **GPU Requirements**: CUDA-compatible hardware for full functionality

## Performance Baselines
- **CPU Baseline**: ~10.8s total execution (2035 chars, perfect quality)
- **GPU Accelerated**: ~2.9s total execution (2035 chars, perfect quality, 3.7x speedup)
- **MAX Graph Hybrid**: ~1.0s total execution (218 chars, meaningful quality, 17x speedup)
  - Encoder only: 47ms (23x faster than CPU encoder alone)
  - Encoder similarity: 99.99% cosine similarity with OpenAI

## Next Phase Performance Targets
- **Hybrid Optimized**: ~1.0s total execution (2035 chars, full quality, 17x speedup)
- **Full MAX Graph**: ~0.5s total execution (2035 chars, target 20-30x speedup)

## Debugging Tools
- **`benchmarks/encoder_feature_debug.py`**: Systematic encoder feature comparison
- **Feature Statistics**: Mean, std, cosine similarity, L2 norm analysis  
- **Cross-framework Validation**: MAX Graph vs OpenAI Whisper feature extraction
- **Tensor Analysis**: First-value comparison, difference patterns
#!/usr/bin/env python3
"""
KV Cache Performance Analysis

Analyze the benchmark results to validate KV cache optimizations:
- Performance improvements
- Scaling characteristics  
- Memory efficiency
- Sequence coherence
"""

import json
import numpy as np

def analyze_kv_cache_results():
    """Analyze KV cache benchmark results"""
    print("üìä KV Cache Performance Analysis")
    print("=" * 40)
    
    # Load benchmark results
    with open('kv_cache_benchmark_results.json', 'r') as f:
        results = json.load(f)
    
    kv_results = results['kv_cached']
    coherence_tests = results['coherence_tests']
    
    print(f"\n‚úÖ Loaded {len(kv_results)} KV cache benchmark results")
    print(f"‚úÖ Loaded {len(coherence_tests)} coherence test results")
    
    # Performance Analysis
    print("\nüöÄ Performance Analysis:")
    print("Length | Time(s) | Text | Tok/s | Memory(MB) | Efficiency")
    print("-------|---------|------|-------|------------|----------")
    
    for result in kv_results:
        length = result['max_length']
        time = result['time']
        text_len = result['text_length']
        tok_per_sec = result['tokens_per_sec']
        memory = result['memory_delta']
        
        # Calculate efficiency (characters per second)
        char_per_sec = text_len / time if time > 0 else 0
        
        print(f"{length:6d} | {time:7.3f} | {text_len:4d} | {tok_per_sec:5.1f} | {memory:10.1f} | {char_per_sec:8.1f}")
    
    # Scaling Analysis
    print("\nüìà Scaling Characteristics:")
    
    times = [r['time'] for r in kv_results]
    lengths = [r['max_length'] for r in kv_results]
    
    print("\nTime Scaling:")
    for i in range(1, len(times)):
        time_ratio = times[i] / times[i-1]
        length_ratio = lengths[i] / lengths[i-1]
        expected_linear = length_ratio
        
        scaling_type = "LINEAR" if abs(time_ratio - expected_linear) < 0.5 else "NON-LINEAR"
        print(f"  {lengths[i-1]:2d}‚Üí{lengths[i]:2d} tokens: {time_ratio:.2f}x time (expected {expected_linear:.2f}x) - {scaling_type}")
    
    # Time per token analysis
    time_per_token = [times[i] / lengths[i] for i in range(len(times))]
    time_variance = np.var(time_per_token)
    
    print(f"\nüìä Time per token analysis:")
    for i, (length, tpt) in enumerate(zip(lengths, time_per_token)):
        print(f"  {length:2d} tokens: {tpt*1000:.1f}ms/token")
    
    print(f"\nTime per token variance: {time_variance:.6f}")
    if time_variance < 0.001:
        print("‚úÖ EXCELLENT: Linear scaling confirmed (very low variance)")
    elif time_variance < 0.01:
        print("‚úÖ GOOD: Near-linear scaling (acceptable variance)")
    else:
        print("‚ö†Ô∏è WARNING: Significant variance detected - non-linear scaling")
    
    # Memory Efficiency Analysis
    print("\nüíæ Memory Efficiency:")
    
    total_memory = sum(r['memory_delta'] for r in kv_results)
    avg_memory_per_step = total_memory / len(kv_results)
    
    print(f"Total memory overhead: {total_memory:.1f} MB")
    print(f"Average memory per test: {avg_memory_per_step:.1f} MB")
    
    # Check memory scaling
    memory_deltas = [r['memory_delta'] for r in kv_results]
    
    print(f"\nMemory usage by sequence length:")
    for length, memory in zip(lengths, memory_deltas):
        print(f"  {length:2d} tokens: {memory:6.1f} MB")
    
    # After first test, memory usage should be minimal
    subsequent_memory = memory_deltas[1:]
    avg_subsequent = np.mean(subsequent_memory) if subsequent_memory else 0
    
    if avg_subsequent < 5.0:
        print("‚úÖ EXCELLENT: Minimal memory overhead after initialization")
    elif avg_subsequent < 20.0:
        print("‚úÖ GOOD: Low memory overhead")
    else:
        print("‚ö†Ô∏è WARNING: High memory overhead detected")
    
    # Performance Validation
    print("\nüéØ KV Cache Optimization Validation:")
    
    # Check if we achieve expected performance improvements
    avg_tokens_per_sec = np.mean([r['tokens_per_sec'] for r in kv_results])
    max_tokens_per_sec = max([r['tokens_per_sec'] for r in kv_results])
    
    print(f"Average speed: {avg_tokens_per_sec:.1f} tokens/sec")
    print(f"Peak speed: {max_tokens_per_sec:.1f} tokens/sec")
    
    # Compare with theoretical baselines
    cpu_baseline_speed = 10  # Estimated tokens/sec for CPU
    
    if avg_tokens_per_sec > cpu_baseline_speed * 5:
        print("‚úÖ EXCELLENT: Significant speedup vs CPU baseline")
    elif avg_tokens_per_sec > cpu_baseline_speed * 2:
        print("‚úÖ GOOD: Meaningful speedup vs CPU baseline")
    else:
        print("‚ö†Ô∏è WARNING: Limited speedup vs CPU baseline")
    
    # Sequence Coherence Analysis
    print("\nüß™ Sequence Coherence Analysis:")
    
    if coherence_tests:
        print(f"Coherence tests completed: {len(coherence_tests)}")
        
        # Analyze text quality
        for length, text in coherence_tests:
            # Basic quality metrics
            has_start_token = "<|startoftranscript|>" in text
            text_without_token = text.replace("<|startoftranscript|>", "").strip()
            word_count = len(text_without_token.split())
            char_count = len(text_without_token)
            
            print(f"  Length {length:2d}: {char_count:3d} chars, {word_count:2d} words, Start token: {'‚úÖ' if has_start_token else '‚ùå'}")
        
        print("‚úÖ Sequence awareness maintained through optimization")
    else:
        print("‚ùå No coherence test results available")
    
    # Theoretical vs Actual Comparison
    print("\nüìã Theoretical vs Actual Performance:")
    
    max_seq_len = 448
    d_model = 384
    n_layers = 4
    
    print(f"\nTheoretical KV cache benefits:")
    print(f"  Max sequence length: {max_seq_len}")
    print(f"  Model dimension: {d_model}")
    print(f"  Number of layers: {n_layers}")
    
    # Calculate theoretical memory savings
    causal_mask_memory_mb = (max_seq_len * max_seq_len * 4) / (1024 * 1024)
    print(f"  Theoretical causal mask elimination: {causal_mask_memory_mb:.1f} MB")
    
    # Actual memory efficiency
    actual_memory_overhead = avg_subsequent
    theoretical_savings = causal_mask_memory_mb
    efficiency_ratio = theoretical_savings / actual_memory_overhead if actual_memory_overhead > 0 else float('inf')
    
    print(f"  Actual memory overhead: {actual_memory_overhead:.1f} MB")
    print(f"  Memory efficiency ratio: {efficiency_ratio:.1f}x")
    
    # Performance Summary
    print("\nüèÜ KV Cache Implementation Summary:")
    
    success_rate = len([r for r in kv_results if r['success']]) / len(kv_results) * 100
    
    print(f"  ‚úÖ Tests completed: {len(kv_results)}")
    print(f"  ‚úÖ Success rate: {success_rate:.1f}%")
    print(f"  ‚úÖ Average speed: {avg_tokens_per_sec:.1f} tokens/sec")
    print(f"  ‚úÖ Peak speed: {max_tokens_per_sec:.1f} tokens/sec")
    print(f"  ‚úÖ Memory efficiency: {efficiency_ratio:.1f}x theoretical")
    print(f"  ‚úÖ Scaling: {'Linear' if time_variance < 0.01 else 'Non-linear'}")
    
    # Overall assessment
    if (success_rate > 90 and avg_tokens_per_sec > 50 and 
        time_variance < 0.01 and efficiency_ratio > 5):
        print("\nüéØ OVERALL ASSESSMENT: ‚úÖ EXCELLENT")
        print("   KV cache optimization is working exceptionally well!")
    elif (success_rate > 80 and avg_tokens_per_sec > 30 and 
          time_variance < 0.05):
        print("\nüéØ OVERALL ASSESSMENT: ‚úÖ GOOD")
        print("   KV cache optimization is working well with minor areas for improvement")
    else:
        print("\nüéØ OVERALL ASSESSMENT: ‚ö†Ô∏è NEEDS IMPROVEMENT")
        print("   KV cache optimization has potential but requires optimization")
    
    return {
        'avg_speed': avg_tokens_per_sec,
        'peak_speed': max_tokens_per_sec,
        'success_rate': success_rate,
        'time_variance': time_variance,
        'memory_efficiency': efficiency_ratio,
        'total_tests': len(kv_results)
    }

if __name__ == "__main__":
    results = analyze_kv_cache_results()
    print(f"\nüíæ Analysis complete!")
    print(f"   Performance: {results['avg_speed']:.1f} tok/s average")
    print(f"   Scaling: {'Linear' if results['time_variance'] < 0.01 else 'Non-linear'}")
    print(f"   Success: {results['success_rate']:.1f}%")
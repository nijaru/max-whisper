#!/usr/bin/env python3
"""
Repetition Pattern Analysis for MAX Graph Whisper
Analyzes attention patterns, decoder states, and feature differences causing repetition
"""

import torch
import numpy as np
import whisper
import json
from pathlib import Path

class RepetitionAnalyzer:
    def __init__(self):
        self.model = whisper.load_model("tiny")
        self.audio_file = "audio_samples/modular_video.wav"
        
    def analyze_cpu_baseline(self):
        """Get baseline CPU transcription with attention patterns"""
        print("üîç Analyzing CPU baseline...")
        
        # Load audio
        audio = whisper.load_audio(self.audio_file)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio).to(self.model.device)
        
        # Get encoder features
        encoder_features = self.model.encoder(mel.unsqueeze(0))
        
        # Full transcription
        result = whisper.transcribe(self.model, self.audio_file)
        
        return {
            "text": result["text"],
            "encoder_features": encoder_features.detach().cpu().numpy(),
            "text_length": len(result["text"]),
            "segments": result.get("segments", [])
        }
    
    def analyze_max_graph_features(self):
        """Analyze MAX Graph encoder features"""
        print("üîç Loading MAX Graph encoder features...")
        
        # This would load the last MAX Graph encoder output
        # For now, let's simulate based on the patterns we've seen
        
        # From the output, we know:
        # MAX Graph: mean=0.0310, std=1.4475
        # OpenAI: mean=0.0002, std=0.4001
        
        max_features = {
            "mean": 0.0310,
            "std": 1.4475,
            "range": [-17.4865, 18.7388],
            "shape": (1, 1500, 384)
        }
        
        openai_features = {
            "mean": 0.0002, 
            "std": 0.4001,
            "range": [-1.7820, 1.7729],
            "shape": (1, 1500, 384)
        }
        
        return max_features, openai_features
    
    def analyze_decoder_behavior(self):
        """Analyze why decoder starts repeating after 200 characters"""
        print("üîç Analyzing decoder repetition patterns...")
        
        max_output = "Max provides several different libraries, including a high-performance serving library that enables you to influence on the most popular Genie I models out of the box on AMD and Nvidia hardware. With support for portability, you can see that you can see that."
        
        # Find repetition point
        repetition_start = max_output.find("you can see that you can see that")
        content_before_repetition = max_output[:repetition_start].strip()
        
        analysis = {
            "total_length": len(max_output),
            "unique_content_length": len(content_before_repetition),
            "repetition_start_char": repetition_start,
            "content_before_repetition": content_before_repetition,
            "repetition_pattern": "you can see that",
            "repetition_count": max_output.count("you can see that")
        }
        
        print(f"üìä Repetition Analysis:")
        print(f"   Total length: {analysis['total_length']} chars")
        print(f"   Unique content: {analysis['unique_content_length']} chars") 
        print(f"   Repetition starts at: {analysis['repetition_start_char']} chars")
        print(f"   Pattern: '{analysis['repetition_pattern']}'")
        print(f"   Repetition count: {analysis['repetition_count']}")
        
        return analysis
    
    def analyze_feature_scaling_impact(self):
        """Analyze how feature scaling affects decoder confidence"""
        print("üîç Analyzing feature scaling impact...")
        
        # From previous experiments, we know:
        scaling_results = {
            "variance_correction_0.6": {
                "output": "repetitive/short",
                "description": "Heavy scaling causes repetition"
            },
            "variance_correction_0.8": {
                "output": "short but coherent", 
                "description": "Moderate scaling causes early stopping"
            },
            "variance_correction_1.0": {
                "output": "semantic + long but repetitive",
                "description": "No scaling preserves semantics but enables repetition"
            }
        }
        
        print("üìä Feature Scaling Impact:")
        for scale, result in scaling_results.items():
            print(f"   {scale}: {result['description']}")
            
        return scaling_results
    
    def suggest_optimization_strategies(self):
        """Suggest strategies to reduce repetition while preserving quality"""
        print("üí° Optimization Strategies:")
        
        strategies = [
            {
                "name": "Decoder Temperature Tuning",
                "description": "Increase temperature (0.2 ‚Üí 0.5) to add randomness",
                "risk": "Low - preserves semantic meaning",
                "implementation": "Modify decode parameters in whisper_max.py"
            },
            {
                "name": "Nucleus Sampling",
                "description": "Use top-p sampling instead of greedy decoding",
                "risk": "Low - standard technique for repetition reduction",
                "implementation": "Add do_sample=True, top_p=0.9 to decoder"
            },
            {
                "name": "Repetition Penalty",
                "description": "Add penalty for repeated n-grams",
                "risk": "Medium - might affect semantic accuracy",
                "implementation": "Custom repetition detection in decoder loop"
            },
            {
                "name": "Attention Pattern Modification",
                "description": "Subtle attention masking to prevent loops",
                "risk": "High - complex to implement without breaking semantics",
                "implementation": "Modify attention computation in decoder"
            },
            {
                "name": "Feature Post-Processing",
                "description": "Apply subtle noise/jitter to encoder features",
                "risk": "Medium - might affect semantic accuracy",
                "implementation": "Add small random perturbation to features"
            }
        ]
        
        for i, strategy in enumerate(strategies, 1):
            print(f"   {i}. {strategy['name']}")
            print(f"      ‚Ä¢ {strategy['description']}")
            print(f"      ‚Ä¢ Risk: {strategy['risk']}")
            print(f"      ‚Ä¢ Implementation: {strategy['implementation']}")
            print()
            
        return strategies
    
    def run_full_analysis(self):
        """Run complete repetition analysis"""
        print("üöÄ Starting Repetition Pattern Analysis")
        print("=" * 50)
        
        # Get baseline
        baseline = self.analyze_cpu_baseline()
        print(f"‚úÖ CPU baseline: {baseline['text_length']} chars")
        
        # Analyze MAX Graph features  
        max_features, openai_features = self.analyze_max_graph_features()
        print(f"‚úÖ Feature analysis complete")
        
        # Analyze decoder behavior
        decoder_analysis = self.analyze_decoder_behavior()
        
        # Analyze scaling impact
        scaling_analysis = self.analyze_feature_scaling_impact()
        
        # Get optimization strategies
        strategies = self.suggest_optimization_strategies()
        
        # Summary
        print("üìã Analysis Summary:")
        print(f"   ‚Ä¢ CPU baseline produces {baseline['text_length']} chars")
        print(f"   ‚Ä¢ MAX Graph produces {decoder_analysis['total_length']} chars")  
        print(f"   ‚Ä¢ Repetition starts at {decoder_analysis['repetition_start_char']} chars")
        print(f"   ‚Ä¢ Unique content: {decoder_analysis['unique_content_length']} chars")
        print(f"   ‚Ä¢ Primary issue: Decoder confidence loss ‚Üí repetition loops")
        print(f"   ‚Ä¢ Root cause: Feature distribution differences despite statistical match")
        
        return {
            "baseline": baseline,
            "max_features": max_features,
            "openai_features": openai_features,
            "decoder_analysis": decoder_analysis,
            "scaling_analysis": scaling_analysis,
            "strategies": strategies
        }

if __name__ == "__main__":
    analyzer = RepetitionAnalyzer()
    results = analyzer.run_full_analysis()
    
    # Save analysis
    output_file = "repetition_analysis.json"
    with open(output_file, "w") as f:
        # Convert numpy arrays to lists for JSON serialization
        serializable_results = {}
        for key, value in results.items():
            if key == "baseline" and "encoder_features" in value:
                value = dict(value)
                value["encoder_features"] = "saved_separately"  # Too large for JSON
            serializable_results[key] = value
            
        json.dump(serializable_results, f, indent=2)
    
    print(f"\n‚úÖ Analysis saved to {output_file}")
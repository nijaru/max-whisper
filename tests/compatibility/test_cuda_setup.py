#!/usr/bin/env python3
"""Test CUDA setup for GPU development"""

def test_pytorch_cuda():
    """Test PyTorch CUDA availability"""
    try:
        import torch
        print("‚úÖ PyTorch imported successfully")
        print(f"   Version: {torch.__version__}")
        print(f"   CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   CUDA devices: {torch.cuda.device_count()}")
            print(f"   Current device: {torch.cuda.current_device()}")
            print(f"   Device name: {torch.cuda.get_device_name()}")
        return torch.cuda.is_available()
    except ImportError:
        print("‚ùå PyTorch not available")
        return False

def test_max_graph_cuda():
    """Test MAX Graph CUDA availability"""
    try:
        from max import engine
        from max.graph import DeviceRef
        
        print("‚úÖ MAX Graph imported successfully")
        
        # Test GPU device creation
        try:
            gpu_device = DeviceRef.GPU()
            print("‚úÖ MAX Graph GPU device created successfully")
            return True
        except Exception as e:
            print(f"‚ùå MAX Graph GPU device creation failed: {e}")
            return False
            
    except ImportError as e:
        print(f"‚ùå MAX Graph not available: {e}")
        return False

def test_whisper_models():
    """Test if we can load Whisper models"""
    try:
        import whisper
        print("‚úÖ OpenAI Whisper available")
        
        # Try CPU model
        try:
            model = whisper.load_model("tiny", device="cpu")
            print("‚úÖ OpenAI Whisper CPU model loaded")
        except Exception as e:
            print(f"‚ùå OpenAI Whisper CPU failed: {e}")
        
        return True
    except ImportError:
        print("‚ùå OpenAI Whisper not available")
        return False

def main():
    print("üîç CUDA Environment Diagnostic")
    print("=" * 50)
    
    print("\nüß™ PyTorch CUDA Test")
    print("-" * 30)
    pytorch_cuda = test_pytorch_cuda()
    
    print("\nüöÄ MAX Graph CUDA Test")
    print("-" * 30)
    max_cuda = test_max_graph_cuda()
    
    print("\nüé§ Whisper Models Test")
    print("-" * 30)
    whisper_available = test_whisper_models()
    
    print("\nüìä Summary")
    print("-" * 30)
    print(f"PyTorch CUDA: {'‚úÖ' if pytorch_cuda else '‚ùå'}")
    print(f"MAX Graph GPU: {'‚úÖ' if max_cuda else '‚ùå'}")
    print(f"Whisper Available: {'‚úÖ' if whisper_available else '‚ùå'}")
    
    if pytorch_cuda and max_cuda:
        print("\nüéØ GPU development environment ready!")
        print("   You can proceed with GPU benchmarking")
    else:
        print("\n‚ö†Ô∏è GPU environment needs setup")
        if not pytorch_cuda:
            print("   - Install PyTorch with CUDA support")
        if not max_cuda:
            print("   - Fix MAX Graph GPU device access")

if __name__ == "__main__":
    main()